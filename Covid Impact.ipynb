{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORTING DATASET(.CSV) INTO PANDAS DATAFRAME\n",
    "import pandas as pd\n",
    "df = pd.read_csv('02_May_12_covid_impact_survey.csv',\n",
    "                       sep=',',\n",
    "                       index_col=False,\n",
    "                       dtype='unicode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#PREPARING CATEGORICAL DATA FOR ENCODER\n",
    "import pandas as pd\n",
    "\n",
    "del df['Unnamed: 0']\n",
    "del df['SU_ID']\n",
    "\n",
    "df['P_PANEL']=df['P_PANEL'].apply(lambda x: 'P_PANEL='+x)\n",
    "# df['NATIONAL_WEIGHT']=df['NATIONAL_WEIGHT'].apply(lambda x: 'NATIONAL_WEIGHT='+x)\n",
    "del df['NATIONAL_WEIGHT']\n",
    "# df['REGION_WEIGHT']=df['REGION_WEIGHT'].apply(lambda x: 'REGION_WEIGHT='+x)\n",
    "del df['REGION_WEIGHT']\n",
    "# df['NATIONAL_WEIGHT_POP']=df['NATIONAL_WEIGHT_POP'].apply(lambda x: 'NATIONAL_WEIGHT_POP='+x)\n",
    "del df['NATIONAL_WEIGHT_POP']\n",
    "# df['REGION_WEIGHT_POP']=df['REGION_WEIGHT_POP'].apply(lambda x: 'REGION_WEIGHT_POP='+x)\n",
    "del df['REGION_WEIGHT_POP']\n",
    "# df['NAT_WGT_COMB_POP']=df['NAT_WGT_COMB_POP'].apply(lambda x: 'NAT_WGT_COMB_POP='+x)\n",
    "del df['NAT_WGT_COMB_POP']\n",
    "# df['REG_WGT_COMB_POP']=df['REG_WGT_COMB_POP'].apply(lambda x: 'REG_WGT_COMB_POP='+x)\n",
    "del df['REG_WGT_COMB_POP']\n",
    "# df['P_GEO']=df['P_GEO'].apply(lambda x: 'P_GEO='+x)\n",
    "del df['P_GEO']\n",
    "df['SOC1']=df['SOC1'].apply(lambda x: 'SOC1='+x)\n",
    "df['SOC2A']=df['SOC2A'].apply(lambda x: 'SOC2A='+x)\n",
    "df['SOC2B']=df['SOC2B'].apply(lambda x: 'SOC2B='+x)\n",
    "df['SOC3A']=df['SOC3A'].apply(lambda x: 'SOC3A='+x)\n",
    "df['SOC3B']=df['SOC3B'].apply(lambda x: 'SOC3B='+x)\n",
    "df['SOC4A']=df['SOC4A'].apply(lambda x: 'SOC4A='+x)\n",
    "df['SOC4B']=df['SOC4B'].apply(lambda x: 'SOC4B='+x)\n",
    "df['PHYS8']=df['PHYS8'].apply(lambda x: 'PHYS8='+x)\n",
    "df['PHYS1A']=df['PHYS1A'].apply(lambda x: 'PHYS1A='+x)\n",
    "df['PHYS1B']=df['PHYS1B'].apply(lambda x: 'PHYS1B='+x)\n",
    "df['PHYS1C']=df['PHYS1C'].apply(lambda x: 'PHYS1C='+x)\n",
    "df['PHYS1D']=df['PHYS1D'].apply(lambda x: 'PHYS1D='+x)\n",
    "df['PHYS1E']=df['PHYS1E'].apply(lambda x: 'PHYS1E='+x)\n",
    "df['PHYS1F']=df['PHYS1F'].apply(lambda x: 'PHYS1F='+x)\n",
    "df['PHYS1G']=df['PHYS1G'].apply(lambda x: 'PHYS1G='+x)\n",
    "df['PHYS1H']=df['PHYS1H'].apply(lambda x: 'PHYS1H='+x)\n",
    "df['PHYS1I']=df['PHYS1I'].apply(lambda x: 'PHYS1I='+x)\n",
    "df['PHYS1J']=df['PHYS1J'].apply(lambda x: 'PHYS1J='+x)\n",
    "df['PHYS1K']=df['PHYS1K'].apply(lambda x: 'PHYS1K='+x)\n",
    "df['PHYS1L']=df['PHYS1L'].apply(lambda x: 'PHYS1L='+x)\n",
    "df['PHYS1M']=df['PHYS1M'].apply(lambda x: 'PHYS1M='+x)\n",
    "df['PHYS1N']=df['PHYS1N'].apply(lambda x: 'PHYS1N='+x)\n",
    "df['PHYS1O']=df['PHYS1O'].apply(lambda x: 'PHYS1O='+x)\n",
    "df['PHYS1P']=df['PHYS1P'].apply(lambda x: 'PHYS1P='+x)\n",
    "df['PHYS1Q']=df['PHYS1Q'].apply(lambda x: 'PHYS1Q='+x)\n",
    "df['SOC5A']=df['SOC5A'].apply(lambda x: 'SOC5A='+x)\n",
    "df['SOC5B']=df['SOC5B'].apply(lambda x: 'SOC5B='+x)\n",
    "df['SOC5C']=df['SOC5C'].apply(lambda x: 'SOC5C='+x)\n",
    "df['SOC5D']=df['SOC5D'].apply(lambda x: 'SOC5D='+x)\n",
    "df['SOC5E']=df['SOC5E'].apply(lambda x: 'SOC5E='+x)\n",
    "df['PHYS2_1']=df['PHYS2_1'].apply(lambda x: 'PHYS2_1='+x)\n",
    "df['PHYS2_2']=df['PHYS2_2'].apply(lambda x: 'PHYS2_2='+x)\n",
    "df['PHYS2_3']=df['PHYS2_3'].apply(lambda x: 'PHYS2_3='+x)\n",
    "df['PHYS2_4']=df['PHYS2_4'].apply(lambda x: 'PHYS2_4='+x)\n",
    "df['PHYS2_5']=df['PHYS2_5'].apply(lambda x: 'PHYS2_5='+x)\n",
    "df['PHYS2_6']=df['PHYS2_6'].apply(lambda x: 'PHYS2_6='+x)\n",
    "df['PHYS2_7']=df['PHYS2_7'].apply(lambda x: 'PHYS2_7='+x)\n",
    "df['PHYS2_8']=df['PHYS2_8'].apply(lambda x: 'PHYS2_8='+x)\n",
    "df['PHYS2_9']=df['PHYS2_9'].apply(lambda x: 'PHYS2_9='+x)\n",
    "df['PHYS2_10']=df['PHYS2_10'].apply(lambda x: 'PHYS2_10='+x)\n",
    "df['PHYS2_11']=df['PHYS2_11'].apply(lambda x: 'PHYS2_11='+x)\n",
    "df['PHYS2_12']=df['PHYS2_12'].apply(lambda x: 'PHYS2_12='+x)\n",
    "df['PHYS2_13']=df['PHYS2_13'].apply(lambda x: 'PHYS2_13='+x)\n",
    "df['PHYS2_14']=df['PHYS2_14'].apply(lambda x: 'PHYS2_14='+x)\n",
    "df['PHYS2_15']=df['PHYS2_15'].apply(lambda x: 'PHYS2_15='+x)\n",
    "df['PHYS2_16']=df['PHYS2_16'].apply(lambda x: 'PHYS2_16='+x)\n",
    "df['PHYS2_17']=df['PHYS2_17'].apply(lambda x: 'PHYS2_17='+x)\n",
    "df['PHYS2_18']=df['PHYS2_18'].apply(lambda x: 'PHYS2_18='+x)\n",
    "df['PHYS2_19']=df['PHYS2_19'].apply(lambda x: 'PHYS2_19='+x)\n",
    "df['PHYS2_DK']=df['PHYS2_DK'].apply(lambda x: 'PHYS2_DK='+x)\n",
    "df['PHYS2_SKP']=df['PHYS2_SKP'].apply(lambda x: 'PHYS2_SKP='+x)\n",
    "df['PHYS2_REF']=df['PHYS2_REF'].apply(lambda x: 'PHYS2_REF='+x)\n",
    "df['PHYS10A']=df['PHYS10A'].apply(lambda x: 'PHYS10A='+x)\n",
    "df['PHYS10B']=df['PHYS10B'].apply(lambda x: 'PHYS10B='+x)\n",
    "df['PHYS10C']=df['PHYS10C'].apply(lambda x: 'PHYS10C='+x)\n",
    "df['PHYS10D']=df['PHYS10D'].apply(lambda x: 'PHYS10D='+x)\n",
    "df['PHYS10E']=df['PHYS10E'].apply(lambda x: 'PHYS10E='+x)\n",
    "df['ECON8A']=df['ECON8A'].apply(lambda x: 'ECON8A='+x)\n",
    "df['ECON8B']=df['ECON8B'].apply(lambda x: 'ECON8B='+x)\n",
    "df['ECON8C']=df['ECON8C'].apply(lambda x: 'ECON8C='+x)\n",
    "df['ECON8D']=df['ECON8D'].apply(lambda x: 'ECON8D='+x)\n",
    "df['ECON8E']=df['ECON8E'].apply(lambda x: 'ECON8E='+x)\n",
    "df['ECON8F']=df['ECON8F'].apply(lambda x: 'ECON8F='+x)\n",
    "df['ECON8G']=df['ECON8G'].apply(lambda x: 'ECON8G='+x)\n",
    "df['ECON8H']=df['ECON8H'].apply(lambda x: 'ECON8H='+x)\n",
    "df['ECON8I']=df['ECON8I'].apply(lambda x: 'ECON8I='+x)\n",
    "df['ECON8J']=df['ECON8J'].apply(lambda x: 'ECON8J='+x)\n",
    "df['ECON8K']=df['ECON8K'].apply(lambda x: 'ECON8K='+x)\n",
    "df['ECON8L']=df['ECON8L'].apply(lambda x: 'ECON8L='+x)\n",
    "df['ECON8M']=df['ECON8M'].apply(lambda x: 'ECON8M='+x)\n",
    "df['ECON8N']=df['ECON8N'].apply(lambda x: 'ECON8N='+x)\n",
    "df['ECON8O']=df['ECON8O'].apply(lambda x: 'ECON8O='+x)\n",
    "df['ECON8P']=df['ECON8P'].apply(lambda x: 'ECON8P='+x)\n",
    "df['ECON8Q']=df['ECON8Q'].apply(lambda x: 'ECON8Q='+x)\n",
    "df['ECON8R']=df['ECON8R'].apply(lambda x: 'ECON8R='+x)\n",
    "df['ECON8S']=df['ECON8S'].apply(lambda x: 'ECON8S='+x)\n",
    "df['ECON7_1']=df['ECON7_1'].apply(lambda x: 'ECON7_1='+x)\n",
    "df['ECON7_2']=df['ECON7_2'].apply(lambda x: 'ECON7_2='+x)\n",
    "df['ECON7_3']=df['ECON7_3'].apply(lambda x: 'ECON7_3='+x)\n",
    "df['ECON7_4']=df['ECON7_4'].apply(lambda x: 'ECON7_4='+x)\n",
    "df['ECON7_5']=df['ECON7_5'].apply(lambda x: 'ECON7_5='+x)\n",
    "df['ECON7_6']=df['ECON7_6'].apply(lambda x: 'ECON7_6='+x)\n",
    "df['ECON7_7']=df['ECON7_7'].apply(lambda x: 'ECON7_7='+x)\n",
    "df['ECON7_8']=df['ECON7_8'].apply(lambda x: 'ECON7_8='+x)\n",
    "df['ECON7_DK']=df['ECON7_DK'].apply(lambda x: 'ECON7_DK='+x)\n",
    "df['ECON7_SKP']=df['ECON7_SKP'].apply(lambda x: 'ECON7_SKP='+x)\n",
    "df['ECON7_REF']=df['ECON7_REF'].apply(lambda x: 'ECON7_REF='+x)\n",
    "df['ECON1']=df['ECON1'].apply(lambda x: 'ECON1='+x)\n",
    "# df['ECON2']=df['ECON2'].apply(lambda x: 'ECON2='+x)\n",
    "del df['ECON2']\n",
    "# df['ECON4']=df['ECON4'].apply(lambda x: 'ECON4='+x)\n",
    "del df['ECON4']\n",
    "df['ECON3']=df['ECON3'].apply(lambda x: 'ECON3='+x)\n",
    "df['ECON4A']=df['ECON4A'].apply(lambda x: 'ECON4A='+x)\n",
    "df['ECON4B']=df['ECON4B'].apply(lambda x: 'ECON4B='+x)\n",
    "df['ECON6A']=df['ECON6A'].apply(lambda x: 'ECON6A='+x)\n",
    "df['ECON6B']=df['ECON6B'].apply(lambda x: 'ECON6B='+x)\n",
    "df['ECON6C']=df['ECON6C'].apply(lambda x: 'ECON6C='+x)\n",
    "df['ECON6D']=df['ECON6D'].apply(lambda x: 'ECON6D='+x)\n",
    "df['ECON6E']=df['ECON6E'].apply(lambda x: 'ECON6E='+x)\n",
    "df['ECON6F']=df['ECON6F'].apply(lambda x: 'ECON6F='+x)\n",
    "df['ECON6G']=df['ECON6G'].apply(lambda x: 'ECON6G='+x)\n",
    "df['ECON6H']=df['ECON6H'].apply(lambda x: 'ECON6H='+x)\n",
    "df['ECON6I']=df['ECON6I'].apply(lambda x: 'ECON6I='+x)\n",
    "df['ECON6J']=df['ECON6J'].apply(lambda x: 'ECON6J='+x)\n",
    "df['ECON6K']=df['ECON6K'].apply(lambda x: 'ECON6K='+x)\n",
    "df['ECON6L']=df['ECON6L'].apply(lambda x: 'ECON6L='+x)\n",
    "df['ECON5A_A']=df['ECON5A_A'].apply(lambda x: 'ECON5A_A='+x)\n",
    "df['ECON5A_B']=df['ECON5A_B'].apply(lambda x: 'ECON5A_B='+x)\n",
    "df['PHYS7_1']=df['PHYS7_1'].apply(lambda x: 'PHYS7_1='+x)\n",
    "df['PHYS7_2']=df['PHYS7_2'].apply(lambda x: 'PHYS7_2='+x)\n",
    "df['PHYS7_3']=df['PHYS7_3'].apply(lambda x: 'PHYS7_3='+x)\n",
    "df['PHYS7_4']=df['PHYS7_4'].apply(lambda x: 'PHYS7_4='+x)\n",
    "df['PHYS7_DK']=df['PHYS7_DK'].apply(lambda x: 'PHYS7_DK='+x)\n",
    "df['PHYS7_SKP']=df['PHYS7_SKP'].apply(lambda x: 'PHYS7_SKP='+x)\n",
    "df['PHYS7_REF']=df['PHYS7_REF'].apply(lambda x: 'PHYS7_REF='+x)\n",
    "df['PHYS11']=df['PHYS11'].apply(lambda x: 'PHYS11='+x)\n",
    "# df['PHYS11_TEMP']=df['PHYS11_TEMP'].apply(lambda x: 'PHYS11_TEMP='+x)\n",
    "del df['PHYS11_TEMP']\n",
    "df['PHYS9A']=df['PHYS9A'].apply(lambda x: 'PHYS9A='+x)\n",
    "df['PHYS9B']=df['PHYS9B'].apply(lambda x: 'PHYS9B='+x)\n",
    "df['PHYS9C']=df['PHYS9C'].apply(lambda x: 'PHYS9C='+x)\n",
    "df['PHYS9D']=df['PHYS9D'].apply(lambda x: 'PHYS9D='+x)\n",
    "df['PHYS9E']=df['PHYS9E'].apply(lambda x: 'PHYS9E='+x)\n",
    "df['PHYS9F']=df['PHYS9F'].apply(lambda x: 'PHYS9F='+x)\n",
    "df['PHYS9G']=df['PHYS9G'].apply(lambda x: 'PHYS9G='+x)\n",
    "df['PHYS9H']=df['PHYS9H'].apply(lambda x: 'PHYS9H='+x)\n",
    "df['PHYS3A']=df['PHYS3A'].apply(lambda x: 'PHYS3A='+x)\n",
    "df['PHYS3B']=df['PHYS3B'].apply(lambda x: 'PHYS3B='+x)\n",
    "df['PHYS3C']=df['PHYS3C'].apply(lambda x: 'PHYS3C='+x)\n",
    "df['PHYS3D']=df['PHYS3D'].apply(lambda x: 'PHYS3D='+x)\n",
    "df['PHYS3E']=df['PHYS3E'].apply(lambda x: 'PHYS3E='+x)\n",
    "df['PHYS3F']=df['PHYS3F'].apply(lambda x: 'PHYS3F='+x)\n",
    "df['PHYS3G']=df['PHYS3G'].apply(lambda x: 'PHYS3G='+x)\n",
    "df['PHYS3H']=df['PHYS3H'].apply(lambda x: 'PHYS3H='+x)\n",
    "df['PHYS3I']=df['PHYS3I'].apply(lambda x: 'PHYS3I='+x)\n",
    "df['PHYS3J']=df['PHYS3J'].apply(lambda x: 'PHYS3J='+x)\n",
    "df['PHYS3K']=df['PHYS3K'].apply(lambda x: 'PHYS3K='+x)\n",
    "df['PHYS3L']=df['PHYS3L'].apply(lambda x: 'PHYS3L='+x)\n",
    "df['PHYS3M']=df['PHYS3M'].apply(lambda x: 'PHYS3M='+x)\n",
    "df['PHYS4']=df['PHYS4'].apply(lambda x: 'PHYS4='+x)\n",
    "df['PHYS5']=df['PHYS5'].apply(lambda x: 'PHYS5='+x)\n",
    "df['PHYS6']=df['PHYS6'].apply(lambda x: 'PHYS6='+x)\n",
    "df['AGE4']=df['AGE4'].apply(lambda x: 'AGE4='+x)\n",
    "df['AGE7']=df['AGE7'].apply(lambda x: 'AGE7='+x)\n",
    "df['GENDER']=df['GENDER'].apply(lambda x: 'GENDER='+x)\n",
    "df['RACETH']=df['RACETH'].apply(lambda x: 'RACETH='+x)\n",
    "df['RACE_R2']=df['RACE_R2'].apply(lambda x: 'RACE_R2='+x)\n",
    "df['HHINCOME']=df['HHINCOME'].apply(lambda x: 'HHINCOME='+x)\n",
    "df['EDUCATION']=df['EDUCATION'].apply(lambda x: 'EDUCATION='+x)\n",
    "df['EDUC4']=df['EDUC4'].apply(lambda x: 'EDUC4='+x)\n",
    "# df['P_OCCUPY2']=df['P_OCCUPY2'].apply(lambda x: 'P_OCCUPY2='+x)\n",
    "del df['P_OCCUPY2']\n",
    "# df['MARITAL']=df['MARITAL'].apply(lambda x: 'MARITAL='+x)\n",
    "del df['MARITAL']\n",
    "# df['LGBT']=df['LGBT'].apply(lambda x: 'LGBT='+x)\n",
    "del df['LGBT']\n",
    "df['HHSIZE1']=df['HHSIZE1'].apply(lambda x: 'HHSIZE1='+x)\n",
    "df['HH01S']=df['HH01S'].apply(lambda x: 'HH01S='+x)\n",
    "df['HH25S']=df['HH25S'].apply(lambda x: 'HH25S='+x)\n",
    "df['HH612S']=df['HH612S'].apply(lambda x: 'HH612S='+x)\n",
    "df['HH1317S']=df['HH1317S'].apply(lambda x: 'HH1317S='+x)\n",
    "df['HH18OVS']=df['HH18OVS'].apply(lambda x: 'HH18OVS='+x)\n",
    "df['REGION4']=df['REGION4'].apply(lambda x: 'REGION4='+x)\n",
    "df['REGION9']=df['REGION9'].apply(lambda x: 'REGION9='+x)\n",
    "df['P_DENSE']=df['P_DENSE'].apply(lambda x: 'P_DENSE='+x)\n",
    "df['MODE']=df['MODE'].apply(lambda x: 'MODE='+x)\n",
    "df['LANGUAGE']=df['LANGUAGE'].apply(lambda x: 'LANGUAGE='+x)\n",
    "# df['MAIL50']=df['MAIL50'].apply(lambda x: 'MAIL50='+x)\n",
    "del df['MAIL50']\n",
    "# df['RACE1_BANNER']=df['RACE1_BANNER'].apply(lambda x: 'RACE1_BANNER='+x)\n",
    "del df['RACE1_BANNER']\n",
    "# df['RACE2_BANNER']=df['RACE2_BANNER'].apply(lambda x: 'RACE2_BANNER='+x)\n",
    "del df['RACE2_BANNER']\n",
    "# df['INC_BANNER']=df['INC_BANNER'].apply(lambda x: 'INC_BANNER='+x)\n",
    "del df['INC_BANNER']\n",
    "# df['AGE_BANNER']=df['AGE_BANNER'].apply(lambda x: 'AGE_BANNER='+x)\n",
    "del df['AGE_BANNER']\n",
    "# df['HH_BANNER']=df['HH_BANNER'].apply(lambda x: 'HH_BANNER='+x)\n",
    "del df['HH_BANNER']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "   P_PANEL_1  P_PANEL_2  SOC1_1  SOC1_2  SOC1_3  SOC1_4  SOC1_5  SOC1_6  \\\n0          1          0       1       0       0       0       0       0   \n1          1          0       1       0       0       0       0       0   \n2          1          0       0       1       0       0       0       0   \n3          1          0       1       0       0       0       0       0   \n4          1          0       1       0       0       0       0       0   \n\n   SOC2A_1  SOC2A_2  ...  REGION9_7  REGION9_8  REGION9_9  P_DENSE_1  \\\n0        1        0  ...          0          0          0          1   \n1        0        1  ...          0          0          0          0   \n2        0        0  ...          0          0          0          1   \n3        0        0  ...          0          0          0          0   \n4        0        0  ...          0          0          0          0   \n\n   P_DENSE_2  P_DENSE_3  MODE_1  MODE_2  LANGUAGE_1  LANGUAGE_2  \n0          0          0       1       0           1           0  \n1          1          0       1       0           1           0  \n2          0          0       1       0           1           0  \n3          1          0       0       1           1           0  \n4          1          0       1       0           1           0  \n\n[5 rows x 803 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P_PANEL_1</th>\n      <th>P_PANEL_2</th>\n      <th>SOC1_1</th>\n      <th>SOC1_2</th>\n      <th>SOC1_3</th>\n      <th>SOC1_4</th>\n      <th>SOC1_5</th>\n      <th>SOC1_6</th>\n      <th>SOC2A_1</th>\n      <th>SOC2A_2</th>\n      <th>...</th>\n      <th>REGION9_7</th>\n      <th>REGION9_8</th>\n      <th>REGION9_9</th>\n      <th>P_DENSE_1</th>\n      <th>P_DENSE_2</th>\n      <th>P_DENSE_3</th>\n      <th>MODE_1</th>\n      <th>MODE_2</th>\n      <th>LANGUAGE_1</th>\n      <th>LANGUAGE_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 803 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder()\n",
    "import category_encoders as ce\n",
    "# df1 =  df.T.apply(lambda x: x.dropna().tolist()).values.tolist()\n",
    "ce_OHE = ce.OneHotEncoder(df)\n",
    "data1 = ce_OHE.fit_transform(df)\n",
    "data1.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "#CONVERTING DATASET INTO TRANSACTIONAL TRUE/FALSE DATAFRAME FOR APRIORI\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "dataset =  df.T.apply(lambda x: x.dropna().tolist()).values.tolist()\n",
    "te = TransactionEncoder()\n",
    "te_ary=te.fit(dataset).transform(dataset)\n",
    "df2 = pd.DataFrame(te_ary, columns=te.columns_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "#EXPORTING PREPARED DATASET AND TRANS-ENCODED DATASET\n",
    "from pathlib import Path\n",
    "filepath1 = Path('C:\\python\\prepared_dataset1.csv')\n",
    "filepath1.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(filepath1)\n",
    "\n",
    "filepath2 = Path('C:\\python\\encoded_dataset1.csv')\n",
    "filepath2.parent.mkdir(parents=True, exist_ok=True)\n",
    "df2.to_csv(filepath2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.48 GiB for an array with shape (3376198, 3, 157) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [49]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#FINDING FREQUENT ITEM SETS USING APRIORI\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmlxtend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfrequent_patterns\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m apriori\n\u001B[1;32m----> 3\u001B[0m frequent_itemsets \u001B[38;5;241m=\u001B[39m \u001B[43mapriori\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf2\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_support\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_colnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m frequent_itemsets\n",
      "File \u001B[1;32mc:\\users\\meisamasgri\\dropbox\\git\\covid19andmentalhealth\\venv\\lib\\site-packages\\mlxtend\\frequent_patterns\\apriori.py:302\u001B[0m, in \u001B[0;36mapriori\u001B[1;34m(df, min_support, use_colnames, max_len, verbose, low_memory)\u001B[0m\n\u001B[0;32m    300\u001B[0m         _bools \u001B[38;5;241m=\u001B[39m _bools \u001B[38;5;241m&\u001B[39m (X[:, combin[:, n]] \u001B[38;5;241m==\u001B[39m all_ones)\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 302\u001B[0m     _bools \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mall(\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcombin\u001B[49m\u001B[43m]\u001B[49m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    304\u001B[0m support \u001B[38;5;241m=\u001B[39m _support(np\u001B[38;5;241m.\u001B[39marray(_bools), rows_count, is_sparse)\n\u001B[0;32m    305\u001B[0m _mask \u001B[38;5;241m=\u001B[39m (support \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m min_support)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 1.48 GiB for an array with shape (3376198, 3, 157) and data type bool"
     ]
    }
   ],
   "source": [
    "#FINDING FREQUENT ITEM SETS USING APRIORI\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "frequent_itemsets = apriori(df2 , min_support=0.001 , use_colnames=True)\n",
    "frequent_itemsets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input DataFrame `df` containing the frequent itemsets is empty.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# FINDING ASSOCIATION RULES WITH CONFIDENCE >0.7\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmlxtend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfrequent_patterns\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m association_rules\n\u001B[1;32m----> 3\u001B[0m rules_confidence \u001B[38;5;241m=\u001B[39m \u001B[43massociation_rules\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfrequent_itemsets\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfidence\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m rules_confidence\n",
      "File \u001B[1;32mc:\\users\\meisamasgri\\dropbox\\git\\covid19andmentalhealth\\venv\\lib\\site-packages\\mlxtend\\frequent_patterns\\association_rules.py:80\u001B[0m, in \u001B[0;36massociation_rules\u001B[1;34m(df, metric, min_threshold, support_only)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"Generates a DataFrame of association rules including the\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;124;03mmetrics 'score', 'confidence', and 'lift'\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     77\u001B[0m \n\u001B[0;32m     78\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m df\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[1;32m---> 80\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe input DataFrame `df` containing \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     81\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthe frequent itemsets is empty.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     83\u001B[0m \u001B[38;5;66;03m# check for mandatory columns\u001B[39;00m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(col \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msupport\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mitemsets\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n",
      "\u001B[1;31mValueError\u001B[0m: The input DataFrame `df` containing the frequent itemsets is empty."
     ]
    }
   ],
   "source": [
    "# FINDING ASSOCIATION RULES WITH CONFIDENCE >0.7\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "rules_confidence = association_rules(frequent_itemsets , metric=\"confidence\" , min_threshold=0.7)\n",
    "rules_confidence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FINDING ASSOCIATION RULES WITH LIFT >1.0\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "rules_lift = association_rules(frequent_itemsets , metric=\"lift\" , min_threshold=1.0)\n",
    "rules_lift"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rules_confidence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [39]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m filepath3 \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124moutput1.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m filepath3\u001B[38;5;241m.\u001B[39mparent\u001B[38;5;241m.\u001B[39mmkdir(parents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 5\u001B[0m \u001B[43mrules_confidence\u001B[49m\u001B[38;5;241m.\u001B[39mto_csv(filepath3)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'rules_confidence' is not defined"
     ]
    }
   ],
   "source": [
    "#EXPORTING RULES\n",
    "from pathlib import Path\n",
    "filepath3 = Path('C:\\python\\output1.csv')\n",
    "filepath3.parent.mkdir(parents=True, exist_ok=True)\n",
    "rules_confidence.to_csv(filepath3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}